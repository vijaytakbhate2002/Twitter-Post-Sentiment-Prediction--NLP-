{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0df76ee0-4420-4f10-99fd-64682b59c76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddc6a7d-79c3-4c7f-a099-be4c98156c77",
   "metadata": {},
   "source": [
    "# Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a374839-4e95-4163-bbab-6915c4b4782a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Tweeter Data ETL\").getOrCreate()\n",
    "\n",
    "class ExtractAbstract(ABC):\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "\n",
    "    def extractCsv(self):\n",
    "        pass\n",
    "\n",
    "    def extractParquet(self):\n",
    "        pass\n",
    "\n",
    "class Extract(ExtractAbstract):\n",
    "\n",
    "    def extractCsv(self):\n",
    "        df = spark.read.option(\"header\", 'true')\\\n",
    "        .option(\"inferSchema\", 'true').\\\n",
    "        csv(self.path)\n",
    "        return df\n",
    "\n",
    "    def extractParquet(self):\n",
    "        df = spark.read.option(\"header\", 'true')\\\n",
    "        .option(\"inferSchema\", 'true').\\\n",
    "        parquet(self.path)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf3faaf4-54ce-40cb-b3fb-9756122a221e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractWorkflow:\n",
    "    def __init__(self, file_format:str):\n",
    "        self.file_format = file_format\n",
    "\n",
    "    def workflow(self, path:str):\n",
    "        \"\"\"Extract data with sepcified format and returns dataframe\"\"\"\n",
    "        extractor = Extract(path=path)\n",
    "        if self.file_format == \"csv\":\n",
    "            data = extractor.extractCsv()\n",
    "        elif self.file_format == \"parquet\":\n",
    "            data = extractor.extractParquet()\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b729a6-10f5-4cd5-8b59-40e2fd5d4e8f",
   "metadata": {},
   "source": [
    "# Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db23c8fe-939f-4b58-b6f6-66ed34d9c44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "\n",
    "class TransformAbstract(ABC):\n",
    "    def __init__(self, df, cols:list):\n",
    "        self.df = df\n",
    "        self.cols = cols\n",
    "\n",
    "    def featureExtraction(self):\n",
    "        pass\n",
    "   \n",
    "    def dropNa(self):\n",
    "        pass\n",
    "\n",
    "    def splitByCatefory(self):\n",
    "        pass\n",
    "   \n",
    "class Transform(TransformAbstract):\n",
    "    def featureExtraction(self):\n",
    "        \"\"\" cols: list (This is list of column names which are considered for data transformation)\n",
    "            Extract Required Features only and returns dataframd\"\"\"\n",
    "        self.df = self.df.select(self.cols)\n",
    "        return self\n",
    "    \n",
    "    def dropNa(self):\n",
    "        \"\"\"Dops rows which contains None value and returns dataframe\"\"\"\n",
    "        self.df = self.df.na.drop()\n",
    "        return self.df\n",
    "    \n",
    "    def splitByCategory(self, target:str) -> dict:\n",
    "        \"\"\" target: str (this is the name of column on which we are applying splitBy)\n",
    "            Accepts target column name (string) and returns dictionary with splitted df by category\"\"\"\n",
    "        print(self.df.show())\n",
    "        print(\"target\", target)\n",
    "        unique_values = [row[target] for row in self.df.select(target).distinct().collect()]\n",
    "        print(f\"columns = {unique_values}\")\n",
    "        temp = {}\n",
    "        for col in unique_values:\n",
    "            temp[col] = self.df[self.df[target] == col]\n",
    "        return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c6809a7-e157-4c5a-8c60-31738ebb0456",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformWorkflow:\n",
    "    def __init__(self, cols:list, target:str):\n",
    "        self.cols = cols\n",
    "        self.target = target\n",
    "\n",
    "    def workflow(self, df) -> dict:\n",
    "        \"\"\"featureExtraction > dropNa > splitByCategory\n",
    "            Returns: tuple (full_df, splitted_by_category_dict)\"\"\"\n",
    "        transformer = Transform(df=df, cols=self.cols)\n",
    "        transformer.featureExtraction()\n",
    "        full_df = transformer.dropNa()\n",
    "        splitted_by_category_dict = transformer.splitByCategory(target=self.target)\n",
    "        print(\"splitted_by_category_dict\", splitted_by_category_dict)\n",
    "        return {\"full_df\":full_df, \"splitted_df\":splitted_by_category_dict}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a69750-24ce-438d-afb9-9f42434c9346",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11b5aa4d-86d1-4fee-830d-a9dd6ff3af17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "from pyspark.sql import DataFrame\n",
    "import os\n",
    "\n",
    "class LoadAbstract(ABC):\n",
    "    def __init__(self, folder_path: str):\n",
    "        self.folder_path = folder_path\n",
    "\n",
    "    def loadData(self, path: str):\n",
    "        pass\n",
    "\n",
    "class Load(LoadAbstract):\n",
    "\n",
    "    def sparkToDf(self, spark_df, cols:list):\n",
    "        temp = []\n",
    "        for row in list(spark_df.collect()[:]):\n",
    "            temp.append(list(row[:]))\n",
    "        df = pd.DataFrame(temp, columns=cols)\n",
    "        return df\n",
    "\n",
    "    def singleCsvLoader(self, df: DataFrame, file_name: str, cols:list):\n",
    "        \"\"\"df (spark dataframe), file_name (string)\n",
    "            Returns: None\n",
    "            Stores df into specified file_name (ex. train.csv)\"\"\"\n",
    "        \n",
    "        path = os.path.join(self.folder_path, file_name) + \".csv\"\n",
    "        df = self.sparkToDf(spark_df=df, cols = cols)\n",
    "        print(f\"Dataframe name = {file_name}\")\n",
    "        df.head()\n",
    "        df.to_csv(path, index=False)\n",
    "    \n",
    "    def multiCsvLoader(self, data: dict, cols:list):\n",
    "        \"\"\"data (dict of spark dataframe and file_name)\n",
    "            Returns: None\n",
    "            Stores each df in 'data' dict into the specified file_name.\"\"\"\n",
    "        \n",
    "        for file_name, df in data.items():       \n",
    "            path = os.path.join(self.folder_path, file_name) + \".csv\"\n",
    "            df = self.sparkToDf(spark_df=df, cols = cols)\n",
    "            print(f\"Dataframe with sentiment {file_name}\")\n",
    "            df.head()\n",
    "            sampler = SampleIndices(y=y)\n",
    "            indices = sampler.getSamples(sample_rate=0.2, stratify=True)\n",
    "            df.iloc[indices].to_csv(path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d6c475b-b899-48b0-9f9e-bae2bdec3f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipe:\n",
    "    TRAIN_PATH = \"data/row_data/twitter_training.csv\"\n",
    "    TEST_PATH = \"data/row_data/twitter_validation.csv\"\n",
    "    OUTPUT_DATA = \"data/processed_data\"\n",
    "\n",
    "    def pipeline_runner(self, file_format:str, cols:list, target:str):\n",
    "        extractor = ExtractWorkflow(file_format=file_format)\n",
    "        train_df = extractor.workflow(path=self.TRAIN_PATH)\n",
    "        test_df = extractor.workflow(path=self.TEST_PATH)\n",
    "\n",
    "        transform_workflow = TransformWorkflow(cols=cols, target=target)\n",
    "        transformed_dict_train = transform_workflow.workflow(df=train_df)\n",
    "        transformed_dict_test = transform_workflow.workflow(df=test_df)\n",
    "\n",
    "        loader = Load(folder_path=self.OUTPUT_DATA)\n",
    "\n",
    "        # Stores full Dataframe\n",
    "        loader.singleCsvLoader(df=transformed_dict_train['full_df'], file_name=\"training\", cols=cols)\n",
    "        loader.singleCsvLoader(df=transformed_dict_test['full_df'], file_name=\"testing\", cols=cols)\n",
    "        \n",
    "        # Stores data with split by category \n",
    "        loader.multiCsvLoader(data=transformed_dict_train['splitted_df'], cols=cols)\n",
    "        loader.multiCsvLoader(data=transformed_dict_test['splitted_df'], cols=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f761f211-6c2c-4cb9-af6d-6ff43b484c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleIndices:\n",
    "    def __init__(self, y):\n",
    "        self.y = y\n",
    "\n",
    "    def stratifiedIndices(self, sample_rate:float) -> dict:\n",
    "        label_indices = []\n",
    "        for label in self.y.unique():\n",
    "            label_indices.append(self.y[self.y == label][:int(sample_rate * len(self.y[self.y == label]))].index.tolist())\n",
    "        label_indices = [val for sublist in label_indices for val in sublist] \n",
    "        return label_indices\n",
    "\n",
    "    def getSamples(self, sample_rate:float=0.7, stratify:bool=True) -> pd.DataFrame:\n",
    "        \"\"\" Sample_rate: float range from 0.1 to 1.0, default=70\n",
    "            statify: bool, default True \"\"\"\n",
    "        if stratify:\n",
    "            label_indices = self.stratifiedIndices(sample_rate)\n",
    "            return label_indices\n",
    "        return self.y.index.tolist()[:int(sample_rate * len(self.y))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b44e0a3-20db-419a-b899-219280da7fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+--------------------+\n",
      "|     topics|sentiments|              tweets|\n",
      "+-----------+----------+--------------------+\n",
      "|Borderlands|  Positive|im getting on bor...|\n",
      "|Borderlands|  Positive|I am coming to th...|\n",
      "|Borderlands|  Positive|im getting on bor...|\n",
      "|Borderlands|  Positive|im coming on bord...|\n",
      "|Borderlands|  Positive|im getting on bor...|\n",
      "|Borderlands|  Positive|im getting into b...|\n",
      "|Borderlands|  Positive|So I spent a few ...|\n",
      "|Borderlands|  Positive|So I spent a coup...|\n",
      "|Borderlands|  Positive|So I spent a few ...|\n",
      "|Borderlands|  Positive|So I spent a few ...|\n",
      "|Borderlands|  Positive|2010 So I spent a...|\n",
      "|Borderlands|  Positive|                 was|\n",
      "|Borderlands|   Neutral|Rock-Hard La Varl...|\n",
      "|Borderlands|   Neutral|Rock-Hard La Varl...|\n",
      "|Borderlands|   Neutral|Rock-Hard La Varl...|\n",
      "|Borderlands|   Neutral|Rock-Hard La Vita...|\n",
      "|Borderlands|   Neutral|Live Rock - Hard ...|\n",
      "|Borderlands|   Neutral|I-Hard like me, R...|\n",
      "|Borderlands|  Positive|that was the firs...|\n",
      "|Borderlands|  Positive|this was the firs...|\n",
      "+-----------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "target sentiments\n",
      "columns = ['Irrelevant', 'Positive', 'Neutral', 'Negative']\n",
      "splitted_by_category_dict {'Irrelevant': DataFrame[topics: string, sentiments: string, tweets: string], 'Positive': DataFrame[topics: string, sentiments: string, tweets: string], 'Neutral': DataFrame[topics: string, sentiments: string, tweets: string], 'Negative': DataFrame[topics: string, sentiments: string, tweets: string]}\n",
      "+--------------------+----------+--------------------+\n",
      "|              topics|sentiments|              tweets|\n",
      "+--------------------+----------+--------------------+\n",
      "|            Facebook|Irrelevant|I mentioned on Fa...|\n",
      "|              Amazon|   Neutral|BBC News - Amazon...|\n",
      "|           Microsoft|  Negative|@Microsoft Why do...|\n",
      "|               CS-GO|  Negative|CSGO matchmaking ...|\n",
      "|              Google|   Neutral|Now the President...|\n",
      "|                FIFA|  Negative|Hi @EAHelp I’ve h...|\n",
      "|           MaddenNFL|  Positive|Thank you @EAMadd...|\n",
      "|TomClancysRainbowSix|  Positive|Rocket League, Se...|\n",
      "|      AssassinsCreed|  Positive|my ass still knee...|\n",
      "|          CallOfDuty|  Negative|FIX IT JESUS ! Pl...|\n",
      "|               Dota2|  Positive|The professional ...|\n",
      "|      AssassinsCreed|  Positive|Itching to assass...|\n",
      "|             Verizon|  Negative|@FredTJoseph hey ...|\n",
      "|               CS-GO|   Neutral|CSGO WIngman (Im ...|\n",
      "|               NBA2K|  Negative|@NBA2K game sucks...|\n",
      "|              Nvidia|  Positive|Congrats to the N...|\n",
      "| GrandTheftAuto(GTA)|  Positive|   yeah and it’s fun|\n",
      "|               Dota2|  Negative|     fuck my life 😆|\n",
      "|RedDeadRedemption...|  Positive|happy birthday re...|\n",
      "|           Microsoft|  Negative|What does that sa...|\n",
      "+--------------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "target sentiments\n",
      "columns = ['Irrelevant', 'Neutral', 'Positive', 'Negative']\n",
      "splitted_by_category_dict {'Irrelevant': DataFrame[topics: string, sentiments: string, tweets: string], 'Neutral': DataFrame[topics: string, sentiments: string, tweets: string], 'Positive': DataFrame[topics: string, sentiments: string, tweets: string], 'Negative': DataFrame[topics: string, sentiments: string, tweets: string]}\n",
      "Dataframe name = training\n",
      "Dataframe name = testing\n",
      "Dataframe with sentiment Irrelevant\n",
      "Dataframe with sentiment Positive\n",
      "Dataframe with sentiment Neutral\n",
      "Dataframe with sentiment Negative\n",
      "Dataframe with sentiment Irrelevant\n",
      "Dataframe with sentiment Neutral\n",
      "Dataframe with sentiment Positive\n",
      "Dataframe with sentiment Negative\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "pipe_runner = Pipe()\n",
    "pipe_runner.pipeline_runner(file_format='csv', cols=[\"topics\", \"sentiments\", \"tweets\"], target='sentiments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c987bd6d-3be4-491f-a3d5-f72f0a98e59d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db671d2-191d-4895-9280-9f731b5a01be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "pyspark_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
